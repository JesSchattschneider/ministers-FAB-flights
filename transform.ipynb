{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "from utils import Wrangling\n",
    "import warnings\n",
    "from unidecode import unidecode\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all_data.csv exists, if it doesn't, run function to create it:\n",
    "if os.path.exists(\"all_data.csv\"):\n",
    "    data = pd.read_csv(\"all_data.csv\", dtype = str)\n",
    "else:\n",
    "    Wrangling.save_tables_from_pdata_cleans()\n",
    "    data = pd.read_csv(\"all_data.csv\", dtype = str)\n",
    "\n",
    "data[\"file_name\"] = data[\"file_name\"].astype(str)# file name as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had issues with tables that share information from multiple ministerios in one row... check 20220218_135419.pdata_clean and the data_to_fix dataframe generated below. These problematic lines are not currently part of the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fix = data[~data[\"file_name\"].str.contains(\"pdata_clean\")] # uncomment this line to check \n",
    "# we could potentially keep these:\n",
    "# data_problem[\"previsao_de_passageiros\"] = data_problem[\"previsao_de_passageiros\"].astype(str)\n",
    "# data_problem = data_problem[~data_problem[\"previsao_de_passageiros\"].str.contains(\"pdata_clean\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing problematic lines from the main dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove problematic data:\n",
    "data[\"file_name\"] = data[\"file_name\"].astype(str)\n",
    "data = data[data[\"file_name\"].str.contains(\"pdf\")]\n",
    "\n",
    "# first round of grooming the dataframe - select specific columns, remove accents, extra spaces \n",
    "cols = [\"autoridades_apoiadas\",\"origem\",\"decolagem_h_local\",\"destino\",\"pouso_h_local\",\"motivo\",\"previsao_de_passageiros\",\"file_name\"]\n",
    "data_clean = Wrangling.clean_flights(data,cols) # to do: combine records removed on this grooming with the data_to_fix\n",
    "\n",
    "# These are the specific grooming to city names:\n",
    "data_clean = data_clean.replace(\"guarulhos\", \"sao paulo\", regex=True)  \n",
    "data_clean = data_clean.replace(\"lisboa\", \"lisbon\", regex=True)  \n",
    "data_clean = data_clean.replace(\"ascension island\", \"wide awake\", regex=True)  \n",
    "data_clean = data_clean.replace(\"port of spain\", \"port-of-spain\", regex=True)  \n",
    "data_clean = data_clean.replace(\"madri\", \"madrid\", regex=True)  \n",
    "data_clean = data_clean.replace(\"gran canaria island\", \"gran canaria\", regex=True)  \n",
    "data_clean = data_clean.replace(\"montevideu\", \"montevideo\", regex=True)  \n",
    "data_clean = data_clean.replace(\"^carajas$\", \"parauapebas\", regex=True)  \n",
    "\n",
    "# select unique - todo: select unique names from \"origem\" and \"destino\" and retrieve result into a vector with the number of times thta a given\n",
    "# city appeared in the flight dataframe\n",
    "unique_origem = data_clean.groupby('origem').nunique().reset_index()[['origem', 'file_name']]\n",
    "unique_destino = data_clean.groupby('destino').nunique().reset_index()[['destino', 'file_name']]\n",
    "# rename cols\n",
    "unique_origem.columns = ['city','count']\n",
    "unique_destino.columns = ['city','count']\n",
    "\n",
    "unique_cities = pd.concat([unique_origem, unique_destino]) # bind origem and destino\n",
    "del(unique_destino, unique_origem, cols, data_to_fix, data) # clean env\n",
    "\n",
    "# group again by unique cities and sum count values\n",
    "unique_cities = unique_cities.groupby('city').sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the airport names and locations from the whole world using the data published by OpenFlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = pd.read_csv(\"airports.csv\")\n",
    "\n",
    "cols_names = [\"airportid\", \"name\", \"city\", \"country\", \"iata\", \"icao\", \"latitude\", \"longitude\", \"altitude\", \"timezone\", \"dst\", \"tz_db\", \"type\", \"source\"]\n",
    "airports.columns = cols_names\n",
    "del(cols_names)\n",
    "\n",
    "# to lower\n",
    "airports[\"city\"] = airports[\"city\"].str.lower()\n",
    "# keep only cols of interest:\n",
    "airports = airports[[\"city\",\"country\",\"latitude\",\"longitude\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge unique cities sourced from FAB flights with airport locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cities_merge = unique_cities.merge(airports, left_on='city', right_on='city', how=\"left\")\n",
    "del(unique_cities)\n",
    "# keep only cities that the location of airports was found in the step above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find cities that matched with only one airport  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need to perform a set of grooming in the unique_cities_merge. For cities that pass the multiple grooming criteria, \n",
    "# we will save those in unique_cities_merge\n",
    "\n",
    "df = unique_cities_merge.loc[unique_cities_merge['country'].notna()] \n",
    "df = df.groupby('city').nunique().reset_index() # count unique \n",
    "\n",
    "# separate cities based on the number of combinations from merge\n",
    "df = df[df['latitude'] == 1]\n",
    "# update country, lat and long columns\n",
    "df = df[[\"city\"]].merge(airports, left_on='city', right_on='city', how=\"left\")\n",
    "\n",
    "# copy to unique_cities_clean\n",
    "unique_cities_clean = df\n",
    "del(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with fixing\n",
    "1- check cities that did not have any matches with the airports from OpenFlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cities that did not have any matches in the merge above\n",
    "df = unique_cities_merge.loc[unique_cities_merge['country'].isna()] \n",
    "\n",
    "# I reviewed all cities that were recorded more than 10 times\n",
    "df = df[df['count'] > 10]\n",
    "df = df[[\"city\"]]\n",
    "\n",
    "## to do: combine again with airports and then with brazilian airports dataframe\n",
    "df = df.merge(airports, left_on='city', right_on='city', how=\"left\")\n",
    "# if cities appear in more than one country take the first option\n",
    "df = df.groupby([\"city\"]).head(1)\n",
    "\n",
    "# keep working with cities that still don't have lat lon and save those that were fixed in this step to the clean df:\n",
    "unique_cities_clean = pd.concat([unique_cities_clean, df.loc[df['latitude'].notna()]])\n",
    "df = df.loc[df['latitude'].isna()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_cities = pd.read_csv(\"brazilian_cities.csv\")\n",
    "br_cities[\"nome\"] = br_cities[\"nome\"].str.lower()\n",
    "br_cities[\"nome\"] = br_cities[\"nome\"].apply(unidecode) # replace letters with accents with the letter without accent.\n",
    "\n",
    "# fix names:\n",
    "\n",
    "# to do: only merge records without lat lon at this point and then group by and select one\n",
    "df = df[[\"city\", \"country\"]].merge(br_cities[[\"nome\", \"latitude\", \"longitude\"]], left_on='city', right_on='nome', how=\"left\")[[\"city\", \"country\", \"latitude\", \"longitude\"]]\n",
    "\n",
    "# country is Brazil\n",
    "df[\"country\"] = \"Brazil\"\n",
    "# if cities appear in more than one state take the first option\n",
    "df = df.groupby([\"city\"]).head(1)\n",
    "\n",
    "# only two cities were not found: praia and ilha do sal, these will be removed from the analysis\n",
    "unique_cities_clean = pd.concat([unique_cities_clean, df.loc[df['latitude'].notna()]])\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - find cities that matched with more than one airport and select just one combination (in case airport appeared more than 10 times in the data dataframe)\n",
    "    - if one of the airports is in Brazil, keep it and remove the others (see vitoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unique_cities_merge.loc[unique_cities_merge['country'].notna()] \n",
    "df = df.groupby('city').nunique().reset_index() # count unique \n",
    "\n",
    "# separate cities based on the number of combinations from merge\n",
    "df = df[df['latitude'] > 1]\n",
    "\n",
    "# # update country, lat and long columns\n",
    "df = df[[\"city\"]].merge(airports, left_on='city', right_on='city', how=\"left\")\n",
    "\n",
    "# # .. and where one of them is in Brazil, choose that one.\n",
    "df_br = df[df['country'].isin([\"Brazil\"])]\n",
    "df_br = df_br.groupby('city').head(1)\n",
    "\n",
    "# # from df, remove cities that were resolved above\n",
    "sel = list(df_br['city']) # select cities that matched with more than one airport\n",
    "df = df[~df['city'].isin(sel)]\n",
    "df = df.groupby('city').head(1)\n",
    "\n",
    "# save to clean df:\n",
    "unique_cities_clean = pd.concat([unique_cities_clean, df, df_br])\n",
    "del(df, df_br, sel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - if more than one airport is in a city in Brazil (see sao paulo), take the first airport location\n",
    " - if none of the airports is in brazil (use column from merge above to remove cities in brazil), group by country + city, select combination with country > airports, merge again with locations and select head\n",
    "\n",
    " https://stackoverflow.com/questions/53842287/select-rows-with-highest-value-from-groupby"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24815c7cfd0f5d2c9970f30cf94da237d5e1f4c89b51e06390960dee5d8cfe9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
