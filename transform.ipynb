{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "from utils import Wrangling\n",
    "import warnings\n",
    "from unidecode import unidecode\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all_data.csv exists, if it doesn't, run function to create it:\n",
    "if os.path.exists(\"all_data.csv\"):\n",
    "    data = pd.read_csv(\"all_data.csv\", dtype = str)\n",
    "else:\n",
    "    Wrangling.save_tables_from_pdfs()\n",
    "    data = pd.read_csv(\"all_data.csv\", dtype = str)\n",
    "\n",
    "data[\"file_name\"] = data[\"file_name\"].astype(str)# file name as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had issues with tables that share information from multiple ministerios in one row... check 20220218_135419.pdf and the data_to_fix dataframe generated below. These problematic lines are not currently part of the analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_fix = data[~data[\"file_name\"].str.contains(\"pdf\")] # uncomment this line to check \n",
    "# we could potentially keep these:\n",
    "# data_problem[\"previsao_de_passageiros\"] = data_problem[\"previsao_de_passageiros\"].astype(str)\n",
    "# data_problem = data_problem[~data_problem[\"previsao_de_passageiros\"].str.contains(\"pdf\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing problematic lines from the main dataframe for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove problematic data:\n",
    "data[\"file_name\"] = data[\"file_name\"].astype(str)\n",
    "data = data[data[\"file_name\"].str.contains(\"pdf\")]\n",
    "\n",
    "# first round of grooming the dataframe - select specific columns, remove accents, extra spaces \n",
    "cols = [\"autoridades_apoiadas\",\"origem\",\"decolagem_h_local\",\"destino\",\"pouso_h_local\",\"motivo\",\"previsao_de_passageiros\",\"file_name\"]\n",
    "data_clean = Wrangling.clean_flights(data,cols) # to do: combine records removed on this grooming with the data_to_fix\n",
    "\n",
    "# select unique - todo: select unique names from \"origem\" and \"destino\" and retrieve result into a vector with the number of times thta a given\n",
    "# city appeared in the flight dataframe\n",
    "unique_origem = data_clean.groupby('origem').nunique().reset_index()[['origem', 'file_name']]\n",
    "unique_destino = data_clean.groupby('destino').nunique().reset_index()[['destino', 'file_name']]\n",
    "# rename cols\n",
    "unique_origem.columns = ['city','count']\n",
    "unique_destino.columns = ['city','count']\n",
    "\n",
    "unique_cities = pd.concat([unique_origem, unique_destino]) # bind origem and destino\n",
    "del(unique_destino, unique_origem, cols, data_to_fix, data) # clean env\n",
    "\n",
    "# group again by unique cities and sum count values\n",
    "unique_cities = unique_cities.groupby('city').sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the airport names and locations from the whole world using the data published by OpenFlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = pd.read_csv(\"airports.csv\")\n",
    "\n",
    "cols_names = [\"airportid\", \"name\", \"city\", \"country\", \"iata\", \"icao\", \"latitude\", \"longitude\", \"altitude\", \"timezone\", \"dst\", \"tz_db\", \"type\", \"source\"]\n",
    "airports.columns = cols_names\n",
    "del(cols_names)\n",
    "\n",
    "# to lower\n",
    "airports[\"city\"] = airports[\"city\"].str.lower()\n",
    "# keep only cols of interest:\n",
    "airports = airports[[\"city\",\"country\",\"latitude\",\"longitude\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge unique cities sourced from FAB flights with airport locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities_airports = unique_cities.merge(airports, left_on='city', right_on='city', how=\"left\")\n",
    "# keep only cities that the location of airports was found in the step above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with fixing\n",
    "1- check cities that did not have any matches with the airports from OpenFlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cities that did not have any matches in the merge above\n",
    "merge_cities_airports_check = merge_cities_airports.loc[merge_cities_airports['country'].isna()] \n",
    "\n",
    "# I reviewed all cities that were recorded more than 10 times\n",
    "merge_cities_airports_check = merge_cities_airports_check[merge_cities_airports_check['count'] > 10]\n",
    "\n",
    "# These are the specific grooming to city names:\n",
    "merge_cities_airports_check = merge_cities_airports_check.replace(\"guarulhos\", \"sao paulo\", regex=True)  \n",
    "merge_cities_airports_check = merge_cities_airports_check.replace(\"lisboa\", \"lisbon\", regex=True)  \n",
    "merge_cities_airports_check = merge_cities_airports_check.replace(\"ascension island\", \"wide awake\", regex=True)  \n",
    "merge_cities_airports_check = merge_cities_airports_check.replace(\"port of spain\", \"port-of-spain\", regex=True)  \n",
    "merge_cities_airports_check = merge_cities_airports_check.replace(\"madri\", \"madrid\", regex=True)  \n",
    "merge_cities_airports_check = merge_cities_airports_check.replace(\"gran canaria island\", \"gran canaria\", regex=True)  \n",
    "merge_cities_airports_check = merge_cities_airports_check.replace(\"montevideu\", \"montevideo\", regex=True)  \n",
    "merge_cities_airports_check = merge_cities_airports_check[[\"city\"]]\n",
    "\n",
    "## to do: combine again with airports and then with brazilian airports dataframe\n",
    "merge_cities_airports_check = merge_cities_airports_check.merge(airports, left_on='city', right_on='city', how=\"left\")\n",
    "merge_cities_airports_check = merge_cities_airports_check.groupby([\"city\"]).head(1)\n",
    "\n",
    "br_cities = pd.read_csv(\"brazilian_cities.csv\")\n",
    "br_cities[\"nome\"] = br_cities[\"nome\"].str.lower()\n",
    "br_cities[\"nome\"] = br_cities[\"nome\"].apply(unidecode) # replace letters with accents with the letter without accent.\n",
    "\n",
    "# to do: only merge records without lat lon at this point and then group by and select one\n",
    "merge_cities_airports_check = merge_cities_airports_check.merge(br_cities, left_on='city', right_on='nome', how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - find cities that matched with only one airport  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cities_airports = merge_cities_airports.loc[merge_cities_airports['country'].notna()] \n",
    "merge_n_combinations = merge_cities_airports.groupby('city').nunique().reset_index() # count unique \n",
    "\n",
    "# separate cities based on the number of combinations from merge\n",
    "merge_one_combination = merge_n_combinations[merge_n_combinations['latitude'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - find cities that matched with more than one airport and select just one combination (in case airport appeared more than 10 times in the data dataframe)\n",
    "    - if one of the airports is in Brazil, keep it and remove the others (see vitoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate cities based on the number of combinations from merge\n",
    "merge_multiple_combinations = merge_n_combinations[merge_n_combinations['latitude'] > 1]\n",
    "\n",
    "# select cities that have multiple airport matches\n",
    "sel = list(merge_multiple_combinations['city']) # select cities that matched with more than one airport\n",
    "merge_multiple_combinations = merge_cities_airports[merge_cities_airports['city'].isin(sel)]\n",
    "\n",
    "# .. and where one of them is in Brazil.\n",
    "merge_multiple_combinations_br = merge_multiple_combinations[merge_multiple_combinations['country'].isin([\"Brazil\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - if more than one airport is in a city in Brazil (see sao paulo), take the first airport location\n",
    " - if none of the airports is in brazil (use column from merge above to remove cities in brazil), group by country + city, select combination with country > airports, merge again with locations and select head\n",
    "\n",
    " https://stackoverflow.com/questions/53842287/select-rows-with-highest-value-from-groupby"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24815c7cfd0f5d2c9970f30cf94da237d5e1f4c89b51e06390960dee5d8cfe9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
